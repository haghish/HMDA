% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hmda.suggest.param.R
\name{hmda.suggest.param}
\alias{hmda.suggest.param}
\title{Suggest Hyperparameters for Tree-based H2O Algorithms}
\usage{
hmda.suggest.param(algorithm, n_models, x = NULL, family = NULL)
}
\arguments{
\item{algorithm}{A character string specifying the H2O algorithm name. Valid inputs are:
\code{"gbm"}, \code{"drf"}, or \code{"xgboost"} (case-insensitive).}

\item{n_models}{An integer specifying the approximate number of total model combinations
desired in the hyperparameter grid. Must be at least 100.}
}
\value{
A named list of hyperparameter value vectors, where each element of the list corresponds
  to a hyperparameter name (e.g., \code{max_depth}, \code{ntrees}, etc.). This list can then be
  passed to H2O's grid search functions.
}
\description{
This function suggests hyperparameter values for tree-based algorithms in the H2O R package.
It currently focuses on three main algorithms:
\itemize{
  \item "gbm" (H2O Gradient Boosting Machine),
  \item "drf" (H2O Distributed Random Forest),
}
It takes a desired number of models (\code{n_models}) and attempts to construct hyperparameter
grids whose Cartesian product is near that size.
}
\details{
\strong{Important}:
\itemize{
  \item If \code{n_models} is less than 100, the function stops with an error.
        This is because \emph{the higher the number of models, the more robust HMDA becomes}.
  \item \code{mtries} in DRF supports a special value of \code{-1}, which means
        "use all features." In other words, if \code{mtries} is set to \code{-1}, H2O will
        include every predictor in each split.
}

The function does not run the grid search. Instead, it returns a list of hyperparameter
values for quick prototyping. Use the returned list with \code{h2o.grid()} or other H2O
training functions.

The strategy used is:
\enumerate{
  \item Define a default set of candidate hyperparameter values for the chosen algorithm.
  \item Calculate the total number of possible combinations (the Cartesian product).
  \item If it exceeds \code{n_models}, prune the largest sets until the total product is near
        or below \code{n_models}.
  \item If it is under \code{n_models} by a wide margin, attempt to add more values (less common).
}
The final size may not match \code{n_models} exactly, but this approach helps avoid extremely large
grids or grids that are too small.
}
\examples{
\dontrun{
  library(h2o)
  h2o.init()

  # Example 1: Suggest hyperparameters for GBM with about 120 models in the grid
  hyperparams_gbm <- hmda.suggest.hyperparameter("gbm", 120)
  hyperparams_gbm

  # Example 2: Suggest hyperparameters for DRF with about 150 models
  hyperparams_drf <- hmda.suggest.hyperparameter("drf", 150)
  hyperparams_drf
  # Notice the special -1 value for mtries, meaning all features are used.

  # Example 3: Suggest hyperparameters for XGBoost with about 200 models
  hyperparams_xgb <- hmda.suggest.hyperparameter("xgboost", 200)
  hyperparams_xgb

  # Then use the suggested hyperparameters in an H2O grid search
  # e.g., for H2O DRF:
  # h2o.grid(
  #   algorithm = "drf",
  #   grid_id = "my_drf_grid",
  #   hyper_params = hyperparams_drf,
  #   training_frame = train,
  #   validation_frame = valid,
  #   x = features,
  #   y = "response_column"
  # )
}

}
