% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hmda.search.param.R
\name{hmda.search.param}
\alias{hmda.search.param}
\title{Search for Hyperparameters via Random Search}
\usage{
hmda.search.param(
  algorithm = c("drf", "gbm"),
  rank = "logloss",
  x,
  y,
  training_frame = h2o.getFrame("hmda.train.hex"),
  validation_frame = NULL,
  max_models = 100,
  fold_assignment = "Modulo",
  nfolds = 10,
  seed = NULL,
  fold_column = NULL,
  weights_column = NULL,
  keep_cross_validation_predictions = TRUE,
  stopping_rounds = NULL,
  stopping_metric = "AUTO",
  stopping_tolerance = NULL,
  ...
)
}
\arguments{
\item{algorithm}{Character vector. The algorithm to include in the
random search. Supported values include "drf" (Distributed
Random Forest) and "gbm" (Gradient Boosting Machine).
The input is case-insensitive.}

\item{rank}{Character string specifying the metric used to rank
models. For metrics not in \code{"logloss",
"mean_per_class_error", "rmse", "mse"}, lower values
indicate better performance; for these four metrics,
higher values are preferred.}

\item{x}{Vector of predictor column names or indices.}

\item{y}{Character string specifying the response column.}

\item{training_frame}{An H2OFrame containing the training data.
Default is \code{h2o.getFrame("hmda.train.hex")}.}

\item{validation_frame}{An H2OFrame for early stopping.
Default is \code{NULL}.}

\item{max_models}{Integer. Maximum number of models to build.
Default is 100.}

\item{fold_assignment}{Character. Method for assigning folds in
cross-validation. Default is "Modulo".}

\item{nfolds}{Integer. Number of folds for cross-validation.
Default is 10.}

\item{seed}{Integer. A seed for reproducibility.
Default is \code{NULL}.}

\item{fold_column}{Character. Column name for cross-validation fold
assignment. Default is \code{NULL}.}

\item{weights_column}{Character. Column name for observation weights.
Default is \code{NULL}.}

\item{keep_cross_validation_predictions}{Logical. Whether to keep
cross-validation predictions. Default is \code{TRUE}.}

\item{stopping_rounds}{Integer. Number of rounds with no improvement
before early stopping. Default is \code{NULL}.}

\item{stopping_metric}{Character. Metric to use for early stopping.
Default is "AUTO".}

\item{stopping_tolerance}{Numeric. Relative tolerance for early stopping.
Default is \code{NULL}.}

\item{...}{Additional arguments passed to \code{h2o.automl()}.}
}
\value{
A list with the following components:
  \describe{
    \item{grid_search}{The H2OAutoML object returned by random search}
    \item{leaderboard}{A merged data frame that combines leaderboard
         performance metrics with hyperparameter settings for each model.
         The data frame is sorted based on the specified ranking metric.}
    \item{hyperparameters_best_of_family}{A summary list of the best
         hyperparameter settings for each performance metric. This strategy
         selects the best model per metric while avoiding duplicate model IDs.}
    \item{hyperparameters_top2}{A list of hyperparameter settings from the
         top 2 models as ranked by the chosen metric.}
    \item{hyperparameters_top5}{A list of hyperparameter settings from the
         top 5 models.}
    \item{hyperparameters_top10}{A list of hyperparameter settings from the
         top 10 models.}
  }
}
\description{
Runs an automated hyperparameter search
  and returns several summaries of the hyperparameter grids as well as
  detailed hyperparameters from each model, and then produces multiple
  summaries based on different strategies. These strategies include:
  \describe{
    \item{Best of Family}{Selects the best model for each performance
         metric (avoiding duplicate model IDs).}
    \item{Top 2}{Extracts hyperparameter settings from the top 2 models
         (according to a specified ranking metric).}
    \item{Top 5}{Extracts hyperparameter settings from the top 5 models.}
    \item{Top 10}{Extracts hyperparameter settings from the top 10 models.}
  }
  These summaries help in identifying candidate hyperparameter ranges
  for further manual tuning. Note that a good suggestion depends on the
  extent of random search you carry out.
}
\details{
The function executes an automated hyperparameter search for the specified
algorithm. It then extracts the leaderboard from the H2OAutoML object and
retrieves detailed hyperparameter information for each model using \code{automlModelParam()} from the
h2otools package. The leaderboard and hyperparameter data are merged by the
\code{model_id} column. Sorting of the merged results is performed based on
the \code{rank} metric. For metrics not in
\code{"logloss", "mean_per_class_error", "rmse", "mse"}, lower values are
considered better; for these four metrics, higher values are preferred.

After sorting, the function applies three strategies to summarize the
hyperparameter search:
\enumerate{
  \item \strong{Best of Family}: Selects the best model for each
        performance metric, ensuring that no model ID appears more than once.
  \item \strong{Top 2}: Gathers hyperparameter settings from the top 2 models.
  \item \strong{Top 5 and Top 10}: Similarly, collects hyperparameter settings
        from the top 5 and top 10 models, respectively.
}
These strategies provide different levels of granularity for analyzing the
hyperparameter space and can be used for prototyping and further manual tuning.
}
\examples{
\dontrun{
  # Initialize H2O (if not already running)
  library(h2o)
  h2o.init()

  # Define predictors and response
  predictors <- c("var1", "var2", "var3")
  response <- "target"

  # Run the hyperparameter search using DRF and GBM algorithms.
  result <- hmda.search.param(algorithm = c("drf"),
                              x = predictors,
                              y = response,
                              training_frame = h2o.getFrame("hmda.train.hex"),
                              max_models = 50,
                              nfolds = 5,
                              stopping_metric = "AUC",
                              stopping_rounds = 3)

  # Access the hyperparameter list of the best_of_family strategy:
  result$best_of_family

  # Access the hyperparameter of the top5 models based on the specified ranking parameter
  result$top5
}

}
